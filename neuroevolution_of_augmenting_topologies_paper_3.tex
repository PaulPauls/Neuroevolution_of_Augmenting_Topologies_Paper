% ######################################################################################################################

\documentclass[journal, a4paper]{IEEEtran}

\usepackage{graphicx}       % For graphics, photos, etc
\usepackage{hyperref}       % For URL and href
\usepackage{amsmath}        % For advanced mathematical formatting and symbols
\usepackage{blindtext}      % For placeholder text
\usepackage{listings}       % For code listings
\usepackage{color}          % For color
\usepackage{draftwatermark} % For watermark

\graphicspath{{./illustrations/}}

\definecolor{green}{rgb}{0, 0.66, 0}
\definecolor{red}{rgb}{1, 0, 0}
\definecolor{gray}{rgb}{0.5, 0.5, 0.5}
\definecolor{orange}{rgb}{1, 0.66, 0}
\definecolor{codebg}{rgb}{0.97, 0.97, 0.97}

\newcommand{\customincludegraphics}[3]{
    \begin{figure}
        \includegraphics[width=0.45\textwidth]{{#1}}
        \caption{{#2}}
        \label{{#3}}
    \end{figure}
}
 
\lstdefinestyle{c-style}{
  language={[ANSI]C},
  frame=single,
  backgroundcolor=\color{codebg},
  commentstyle=\itshape\color{green},
  keywordstyle=\color{blue},
  numberstyle=\tiny\color{gray},
  stringstyle=\color{orange},
  basicstyle=\fontsize{7}{7}\ttfamily,
  breakatwhitespace=false,
  breaklines=true,
  captionpos=b,
  keepspaces=true,
  numbers=left,
  numbersep=5pt,
  showspaces=false,
  showstringspaces=false,
  showtabs=false,
  tabsize=2
}

% ######################################################################################################################

\begin{document}

\title{Neuroevolution of Augmenting Topologies}
\author{Paul Pauls\\
        Advisor: Michael Adam}
\markboth{Neuroevolution of Augmenting Topologies}{}
\maketitle

% Place small Watermark indicating that this is currently a draft in background
\SetWatermarkText{DRAFT}
\SetWatermarkScale{0.5}

% While Paper is in development shall I include this table of contents as a quick overview
\tableofcontents

\begin{abstract}
    \blindtext
\end{abstract}

% ######################################################################################################################

\section{Introduction}

\IEEEPARstart{T}{his} shall be my introduction. And this shall be my citation \cite{cite01}.
\blindtext



% ######################################################################################################################

\section{Neuroevolution and Evolutionary Algorithms}
% Check out the research done by Uber-Research


Neuroevolution is a machine learning technique that applies evolutionary algorithms to construct artificial neural networks, taking inspiration from the evolution of biological nervous systems in nature. \cite{cite02}

A evolutionary algorithm is a generic population-based and meta-heuristically optimized algorithmic solution to an applied problem. When breaking this down to simpler terms, does this mean first of all that an evolutionary algorithm (short form: \textbf{EA}) is simply a solution to an applied problem. This solution can take the form of a classical calculation algorithm or more complex forms like the aforementioned artificial neural network. As generic as its method of solving the problem is, is its application domain. Evolutionary Algorithms - as well as Neuroevolution by which they are employed - are highly general and allow for learning without explicit targets even if provided with only minimal feedback. \cite{cite02}

Evolutionary Algorithms are population-based, meaning they not only handle a single solution to the problem they are applied to, but they have a multitude of solutions to this problem. Each of the solutions is called a \textit{member} of its arbitrarily large population and each solution is arbitrarily similar to one another. Each member of the population is judged by a common \textit{fitness function}, which numerically expresses the members quality of its solution to the applied problem. The higher the corresponding \textit{fitness function} score of a member, the better is the solution to the applied problem. All members are judged by the same fitness function, which is the key hyperparameter that determines how well a problem has been solved.

The key aspect of evolutionary algorithms however lies in its meta-heuristic optimization method. This optimization method improves the members of the population in the sense that along the evolutionary process almost all members of the population score an increasingly higher fitness function evaluation score - meaning they get increasingly better at solving the problem they are applied to.
This optimization method is employed after each \textit{generation} of a population. A generation in the evolutionary process is completed once every member of the population has been applied to the problem and has been assigned a fitness score by the common fitness function. The population is then mixed up through \textit{reproduction}, \textit{mutation}, \textit{recombination} and \textit{selection}. These processes spread traits of high-performing members to low-performing members, preserve high-performing members while extinguishing low-performing members and introduce novel traits to already high-performing members to further explore the solution-space.

Applying the attributes of Evolutionary Algorithms to artificial Neural Networks does result in the machine learning technique Neuroevolution. Neuroevolution algorithms start out with an initial population of neural networks of varying complexity - meaning topology and weights. The exact nature of this initial population is not stipulated by the Neuroevolution technique and depends on the specific Neuroevolution algorithm. In each generation does each neural network member perform input inference and is subsequently judged by the fitness function. The best scoring neural networks spread their traits to lower scoring neural network. Fixed-Topology Neuroevolution algorithms solely adjust the weights of the lower scoring neural networks towards the higher scoring networks while Topology and Weight Evolving Artificial Neural Network algorithms (short form: \textbf{TWEANNs}) additionally also adjust the topology. They introduce new nodes and connections or delete nodes and connections in the low scoring neural networks depending on if they are present in the better scoring neural networks or introduce completely novel nodes and connections in order to explore new, possibly even better performing, solutions.
Through this do neural networks developed through Neuroevolution algorithms often have arbitrary neural models and network structures and are able to progress in learning even without explicit targets and only sparse feedback.

While concluding this introduction to Neuroevolution can be said that Neuroevolution can be classified as part of the machine learning paradigm of \textit{Reinforcement Learning}. Neuroevolution algorithms learn through ever-changing interaction with the environment instead of a predetermined set of input-output mapping or creative self-organization of input through probabilistic models. Neuroevolution algorithms learn from the consequence of their actions on the basis of their past experiences and also by new choices - quite accurately conforming with the definition of Reinforcement learning.


\subsection{Evolutionary Algorithms}


\subsection{Neuroevolution}


\subsection{Landmark Research in Neuroevolution}
% https://www.oreilly.com/ideas/neuroevolution-a-different-kind-of-deep-learning
% Past and Current Research in Neuroevolution (Also specify the explicit Neuroevolution algorithms when getting to the specific research (NEAT, HyperNEAT, DeepNeurevolution, etc))
% Include comparison of landmark research with other Reinforcement Learning Techniques (Deep Q-Learning, etc). I am thinking here especiall about Real, et al 2019







% ######################################################################################################################

\section{NeuroEvolution of Augmenting Topologies (NEAT)}

<Section Introduction>

At time of envisioning of NEAT was Neuroevolution most promising learning approach. Still is powerful today (see rea17/19)
"NE is a promising approach to learning behavioral policies and finds solutions faster than leading RL methods on many benchmark tasks (Gomez 2003; Moriarty and Miikkulainen 1997)" \cite{sta04}


"In highly complex domains the heuristics for determining the appropriate size are not very useful, and it becomes increasingly difficult to solve such domains with fixed-length encodings." \cite{sta04}

[See all notes write down about stanleys PhD thesis]




\subsection{Key Aspects of NEAT and Differences to Preceding Neuroevolution}
% Solving Competing Convetions, Speciation, Historical Markings, Minimal Initial Pop, See Key Elements identified through ablation (http://nn.cs.utexas.edu/downloads/papers/stanley.cec02.pdf)

\subsection{Performance of NEAT}
% Include traditional NEAT performance but also NEAT performance in more current examples

\subsection{Variants and Advancements of NEAT}
% Follow Up Research and Variants (HyperNeat, ES-HyperNeat, Novelty Search, but also small variations of NEAT such as aging NEAT or rtNEAT)
% Mention non-mating variants of neuro evolution algorithms /neat (such as the current research in rea17/19). See chapter 2.3.2 'non-mating' in [sta04] for an overview

\subsubsection{<Variant 1>}
\subsubsection{<Variant 2>}
\subsubsection{<Variant 3>}



% ######################################################################################################################

\section{Practical Applications of NEAT}
% Go especially into detail what Stanley considered great NEAT applications in his reddit AMA
% Introduce my own code accompanying this paper

\subsection{<Application 1>}
\subsection{<Application 2>}
\subsection{<Application 3>}




% ######################################################################################################################

\section{Conclusion}

\blindtext



% ######################################################################################################################

\begin{thebibliography}{5}

  \bibitem{yao99}
    Yao - Evolving Artificial Neural Networks; 1999;
    \url{http://avellano.fis.usal.es/~lalonso/compt_soft/articulos/yao99evolving.pdf}

  \bibitem{sta02_1}
    Stanley, Miikkulainen - Efficient Evolution of Neural Network Topologies; 2002;
    \url{http://nn.cs.utexas.edu/downloads/papers/stanley.cec02.pdf}

  \bibitem{sta02_2}
    Stanley, Miikkulainen - Evolving Neural Networks through Augmented Topologies; 2002;
    \url{http://nn.cs.utexas.edu/downloads/papers/stanley.ec02.pdf}

  \bibitem{gea03}
    Geard, Wiles - Structure and Dynamics of a Gene Network Model Incorporating Small RNAs; Dec 2003;
    \url{https://ieeexplore.ieee.org/document/1299575}

  \bibitem{sta04}
    Stanley - Efficient Evolution of Neural Networks through Complexification; Aug 2004;
    \url{http://nn.cs.utexas.edu/downloads/papers/stanley.phd04.pdf}

  \bibitem{rei07}
    Reisinger, Miikkulainen - Acquiring Evolvability through Adaptive Representations; Jul 2007;
    \url{http://nn.cs.utexas.edu/downloads/papers/reisinger.gecco07.pdf}

  \bibitem{mat07}
    Mattiussi, Duerr, et al - Center of Mass Encoding: A self-adaptive representation with adjustable redundancy for real-valued parameters; Jul 2007;
    \url{https://infoscience.epfl.ch/record/101405}

  \bibitem{flo08}
    Floreano, Duerr, et al - Neuroevolution: From Architectures to Learning; Jan 2008;
    \url{http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.182.1567}

  \bibitem{mat08}
    Mattiussi, Marbach, et al - The Age of Analog Networks; Sep 2008;
    \url{https://www.aaai.org/ojs/index.php/aimagazine/article/view/2156}

  \bibitem{sta09}
    Stanley, Dâ€™Ambrosio, et al - A Hypercube-Based Indirect Encoding for Evolving Large-Scale Neural Networks; 2009;
    \url{http://axon.cs.byu.edu/~dan/778/papers/NeuroEvolution/stanley3**.pdf}

  \bibitem{ris11}
    Risi, Stanley - Enhancing ES-HyperNEAT to Evolve More Complex Regular Neural Networks; Jul 2011;
    \url{http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.365.4332}

  \bibitem{leh11}
    Lehman, Stanley - Novelty Search and the Problem with Objectives; Oct 2011;
    \url{https://www.cs.ucf.edu/eplex/papers/lehman_gptp11.pdf}

  \bibitem{woe12}
    Woergoetter, Porr - Scholarpedia Article on 'Reinforcement Learning'; Sep 2012;
    \url{http://www.scholarpedia.org/article/Reinforcement_learning}

  \bibitem{hol12}
    Holland - Scholarpedia Article on 'Genetic Algorithms'; Oct 2012;
    \url{http://www.scholarpedia.org/article/Genetic_algorithms}

  \bibitem{fog13}
    Fogel, Fogel, et al - Scholarpedia Article on 'Evolutionary Programming'; Oct 2013;
    \url{http://www.scholarpedia.org/article/Evolutionary_programming}

  \bibitem{leh13}
    Lehman, Miikkulainen - Scholarpedia Article on 'Neuroevolution'; Oct 2013;
    \url{http://www.scholarpedia.org/article/Neuroevolution}

  \bibitem{pas14}
    Pascanu, Ganguli, et al - On the Saddle Point for Non-Convex Optimization; May 2014;
    \url{https://www.researchgate.net/publication/262452520}

  \bibitem{kim15}
    Kim, Rigazio - Deep Clustered Convolutional Kernels; Mar 2015;
    \url{https://arxiv.org/abs/1503.01824}

  \bibitem{fer16}
    Fernando, Banarse, et al - Convolution by Evolution; Jun 2016;
    \url{https://arxiv.org/abs/1606.02580}

  \bibitem{mii17}
    Miikkulainen, Liang, et al - Evolving Deep Neural Networks; Mar 2017;
    \url{https://arxiv.org/abs/1703.00548}

  \bibitem{xie17}
    Xie, Yuille - Genetic CNN; Mar 2017;
    \url{https://arxiv.org/abs/1703.01513}

  \bibitem{neg17}
    Negrinho, Gordon - DeepArchitect: Automatically Designing and Training Deep Architectures; Apr 2017;
    \url{https://arxiv.org/abs/1704.08792}

  \bibitem{rea17}
    Real, Moore, et al - Large-scale Evolution of Image Classifiers; Jun 2017;
    \url{https://arxiv.org/abs/1703.01041}

  \bibitem{sta17_neuroevolution_overview}
    Stanley - Neuroevolution: A Different Kind of Deep Learning; Jul 2017;
    \url{https://www.oreilly.com/ideas/neuroevolution-a-different-kind-of-deep-learning}

  \bibitem{bro17}
    Brock, Lim, et al - SMASH: One-Shot Model Architecture Search through HyperNetworks; Aug 2017;
    \url{https://arxiv.org/abs/1708.05344}

  \bibitem{sal17}
    Salimans, Ho - Evolution Strategies as a Scalable Alternative to Reinforcement Learning; Sep 2017;
    \url{https://arxiv.org/abs/1703.03864}

  \bibitem{jad17}
    Jaderberg, Dalibard, et al - Population Based Training of Neural Networks; Nov 2017;
    \url{https://arxiv.org/abs/1711.09846}

  \bibitem{zha17}
    Zhang, Clune, et al - On the Relationship Between the OpenAI Evolution Strategy and Stochastic Gradient Descent; Dec 2017;
    \url{https://arxiv.org/abs/1712.06564}

  \bibitem{sta17_deep_neuroevolution}
    Stanley, Clune - Welcoming the Era of Deep Neuroevolution; Dec 2017;
    \url{https://eng.uber.com/deep-neuroevolution/}

  \bibitem{liu18}
    Liu, Simonyan, et al - Hierarchical Representation for Efficient Architecture Search; Feb 2018;
    \url{https://arxiv.org/abs/1711.00436}

  \bibitem{suc18_accelerating_deep_neuroevolution}
    Such, Stanley, et al - Accelerating Deep Neuroevolution: Train Atari in Hours on a Single Personal Computer; Apr 2018;
    \url{https://eng.uber.com/accelerated-neuroevolution/}

  \bibitem{suc18_introduction_deep_neuroevolution}
    Such, Madhavan, et al - Deep Neuroevolution: Genetic Algorithms Are a Competitive Alternative for Training Deep Neural Networks for Reinforcement Learning; Apr 2018;
    \url{https://arxiv.org/abs/1712.06567}

  \bibitem{zop18}
    Zoph, Vasudevan, et al - Learning Transferable Architectures or Scalable Image Recognition; Apr 2018;
    \url{https://arxiv.org/abs/1707.07012}

  \bibitem{leh18_evolution_strategy}
    Lehman, Chen, et al - ES Is More Than Just a Traditional Finite-Difference Approximator; May 2018;
    \url{https://arxiv.org/abs/1712.06568}

  \bibitem{leh18_safe_mutations}
    Lehman, Chen, et al - Safe Mutations for Deep and Recurrent Neural Networks through Output Gradients; May 2018;
    \url{https://arxiv.org/abs/1712.06563}

  \bibitem{zho18}
    Zhong, Yan, et al - Practical Block-Wise Neural Network Architecture Generation; May 2018;
    \url{https://arxiv.org/abs/1708.05552}

  \bibitem{raw18}
    Rawal, Miikkulainen - From Nodes to Networks: Evolving Recurrent Neural Networks; Jun 2018;
    \url{https://arxiv.org/abs/1803.04439}

  \bibitem{con18}
    Conti, Madhavan, et al - Improving Exploration in Evolution Strategies for Deep Reinforcement Learning via a Population of Novelty Seeking Agents; Oct 2018;
    \url{https://arxiv.org/abs/1712.06560}

  \bibitem{rea19}
    Real, Aggarwal, et al - Regularized Evolution for Image Classifier Architecture Search; Feb 2019;
    \url{https://arxiv.org/abs/1802.01548}

  \bibitem{sun19}
    Sun, Xue, et al - Evolving Deep Convolutional Neural Networks for Image Classification; Mar 2019;
    \url{https://arxiv.org/abs/1710.10741}



\end{thebibliography}

\end{document}
